{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Torob Project: Product Category Classification Based On Images\n",
        "\n",
        "Hi and welcome to Torob hands-on project for OpenAI machine learning bootcamp 2023! 👋 🎉\n",
        "\n",
        "In this project, you will use the real images of a very small subset of products in Torob product catalogue for the task of \"product category classification\". Given a set of product images from 10 different categories, you should build a classifier model which given a product image, it would predict the category of the product.\n",
        "\n",
        "**Note that the sections which you need to complete or write your answers are marked with 🔵 (a blue circle).**\n",
        "\n",
        "There is also a Persian guide for this project which you can access here: https://docs.google.com/document/d/1FiWXlNUo44U1ECRG7OSKJXsC_WjMjcMSPfz7lMhbYzo/edit?usp=sharing\n",
        "\n",
        "So, let's get started!"
      ],
      "metadata": {
        "id": "jJCym9kcC_Xf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⭕ Install dependencies\n",
        "\n",
        "▶ Since we are going to work with HuggingFace Transformers library, it needs to be installed. Further, the `gdown` and `datasets` packages are also installed which are used for downloading the data from Google Drive and loading and reading the data, respectively."
      ],
      "metadata": {
        "id": "ozfLEFO4Ce16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U datasets gdown transformers"
      ],
      "metadata": {
        "id": "SCtkaINYdFvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶ And then we import the necessary libraries and modules:"
      ],
      "metadata": {
        "id": "V2_5YYcDdzWa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6yguB8VAV2Z"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoImageProcessor, ResNetModel\n",
        "import torch\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⭕ Download Data\n",
        "\n",
        "▶ As the first step, we need to download the data of product images. For your convenience, it has been uploaded as a zip file on the Google Drive and can be downloaded using `gdown` command. Just run the following cell to download the data."
      ],
      "metadata": {
        "id": "acliq2FZW6FR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1DM8cKZFFU1z5fN7h34l3Ji6h5UytYMpb"
      ],
      "metadata": {
        "id": "a6J2MJw4XhhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⭕ Load Data\n",
        "\n",
        "▶ Next, we should load the image data. The structure of the data is as follows:\n",
        "\n",
        "```\n",
        "torob-product-images\n",
        "    fryer\n",
        "        1.jpg\n",
        "        2.jpg\n",
        "        ...\n",
        "    graphic-card\n",
        "        1.jpg\n",
        "        2.jpg\n",
        "        ...\n",
        "    ...\n",
        "```\n",
        "▶ The images for each category is in a seperate directory. We use the `load_dataset` function from HuggingFace `dataset` library to load all the images, and use the name of each directory as the label of the images."
      ],
      "metadata": {
        "id": "zVp-zo80XkgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"imagefolder\", data_files=\"./torob-product-images.zip\", split='train')"
      ],
      "metadata": {
        "id": "Li74tKW4ZtFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶ Let's inspect the labels of the images to see the title of the 10 categories:"
      ],
      "metadata": {
        "id": "EGIFU3hGcRgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.features['label'].names"
      ],
      "metadata": {
        "id": "-SCGO6EbCznR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶ Further, note that the images can be accessed via `\"images\"` column of the dataset (i.e. `dataset[\"images\"]`) and their corresponding labels can be accessed using `\"label\"` column (i.e. `dataset[\"label\"]`)."
      ],
      "metadata": {
        "id": "BaHED74C7tza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶ Let's randomly select a few images and display them (the resized images are displayed here, because some of them may be big)."
      ],
      "metadata": {
        "id": "XI0PAUTOaTq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_idx = random.sample(range(len(dataset)), k=10)\n",
        "sampled_data = dataset.select(sampled_idx)\n",
        "for img, lbl in zip(sampled_data[\"image\"], sampled_data[\"label\"]):\n",
        "  print(\"Label of following image:\", dataset.features['label'].names[lbl])\n",
        "  display(img.resize((200, 200)))\n",
        "  print(\"=\"*80)"
      ],
      "metadata": {
        "id": "hXSN6FttcOqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⭕ Train-Test Split\n",
        "\n",
        "▶ We need to split our data into train and test set so that we can evaluate the performance of different classifiers on unseen data. We use a 80/20 split (80% for train, 20% for test).\n",
        "\n",
        "🔵 To split the dataset, we can use `train_test_split` method (it's very similar to `train_test_split` function in `sklearn` library). Replace the `???` in the following cell."
      ],
      "metadata": {
        "id": "gDvslRf_k7JK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splited_dataset = dataset.???(test_size=???, stratify_by_column='label', shuffle=True, seed=42)\n",
        "train_dataset = splited_dataset[\"train\"]\n",
        "test_dataset = splited_dataset[\"test\"]"
      ],
      "metadata": {
        "id": "jZ-VrjY8dZBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⭕ Simple Features Extraction\n",
        "\n",
        "▶ The first method we use for feature extraction is very simple: for each image, the **average** (mean) and **standard deviation** (std) of pixels of each channel (R, G & B) is calculated, and together they are used to represent that image. In other words, each image would be represented by only 6 (3 for average + 3 for standard deviation) numbers.\n",
        "\n",
        "🔵 We have defined a function named `calculate_simple_features` but it's not complete. It operates on single example of the dataset to calculate the features mentioned above and stores the features in the key `\"simple_features\"`. Use `numpy` library (imported as `np`) to calculate the average and standard deviation of each image. Replace all the `???` to complete the implementation."
      ],
      "metadata": {
        "id": "CLyQEytRVHBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_simple_features(example):\n",
        "  img_mean = np.???(example[\"image\"], axis=(0, 1))\n",
        "  img_std = np.???(example[\"image\"], axis=(0, 1))\n",
        "  example[\"simple_features\"] = np.concatenate((???, ???), axis=-1)\n",
        "  return example"
      ],
      "metadata": {
        "id": "kaH6oQA5Bf1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔵 Now, use `map` method of the dataset to apply the above function on each example in the train and test datasets. Replace `???` with your answers."
      ],
      "metadata": {
        "id": "KlP7hMyypTjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ???_dataset.???(calculate_simple_features)\n",
        "test_dataset = ???_dataset.???(calculate_simple_features)"
      ],
      "metadata": {
        "id": "Ap7F5Eux_1A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶ Make sure we have correctly implemented the feature extraction step. We should see \"Well done!\" printed without any errors."
      ],
      "metadata": {
        "id": "uruHXyXlqANt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(train_dataset[\"simple_features\"][0]) == 6\n",
        "assert len(test_dataset[\"simple_features\"][0]) == 6\n",
        "print(\"Well done!\")"
      ],
      "metadata": {
        "id": "WnS9jqU4_1H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⭕ Train and Evaluate Models on Simple Features\n",
        "\n",
        "▶ Now, it's time to train and evaluate some models on the extracted features. We try three different classifiers: KNN, Decision Tree, and Random Forest."
      ],
      "metadata": {
        "id": "2a2NJfvhqyPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔵 KNN Classifier\n",
        "\n",
        "Train a **KNN** classifier on the extracted features for the training data. We can use the `sklearn` library for this purpose.\n",
        "\n",
        "You can play with the hyper-parameters of the classifier, e.g. `n_neighbors`, and tune them (but it's not necessary).\n",
        "\n",
        "**Hint:** Remember that the features are stored in `\"simple_features\"` column of the dataset (i.e. `train_dataset[\"simple_features\"]`), and the labels are stored in `\"label\"` column."
      ],
      "metadata": {
        "id": "JGr7IXLjLYgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn_classifier = ???\n",
        "???"
      ],
      "metadata": {
        "id": "yo7r2L0k_1KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔵 Now, find the accuracy of trained classifier on the test data.\n",
        "\n",
        "**Hint:** You can use `score` method."
      ],
      "metadata": {
        "id": "P-StHBa6L3ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn_classifier.???(???)"
      ],
      "metadata": {
        "id": "7tCZf2C8LNxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔵 Decision Tree Classifier\n",
        "\n",
        "Next, we train a **decision tree** classifier on the extracted features for the training data. We can use the `sklearn` library for this purpose.\n",
        "\n",
        "You can play with the hyper-parameters of the classifier, e.g. `max_depth`, and tune them (but it's not necessary)."
      ],
      "metadata": {
        "id": "jAasMLnLQuu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_classifier = ???\n",
        "???"
      ],
      "metadata": {
        "id": "t-9Hi7ra_1Sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔵 Like before, we would like to find the accuracy of trained classifier on the test data.\n",
        "\n",
        "**Hint:** Still you can use `score` method!"
      ],
      "metadata": {
        "id": "9jqTPnxiRULb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_classifier.???(???)"
      ],
      "metadata": {
        "id": "lE263jP0LQIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔵 Random Forest Classifier\n",
        "\n",
        "And another classifier: we train a **random forest** classifier on the extracted features for the training data. We can use the `sklearn` library for this purpose.\n",
        "\n",
        "You can play with the hyper-parameters of the classifier, e.g. `n_estimators`, and tune them (but it's not necessary)."
      ],
      "metadata": {
        "id": "SREM9TG2Rf9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_classifier = ???\n",
        "???"
      ],
      "metadata": {
        "id": "bBFzPCSvLASJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔵 And again we need to find the accuracy of trained classifier on the test data."
      ],
      "metadata": {
        "id": "-e9sBf7jtSgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_classifier.???(???)"
      ],
      "metadata": {
        "id": "KFo7DgmGLS8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⭕ Histogram Features Extraction\n",
        "\n",
        "▶ The second method we use for feature extraction is more complex than the first method: for each image, the histogram of pixel values is calculated, and this histogram is used to represent that image. In other words, each image would be represented by *b* numbers, where *b* is the number of bins in the histogram. We expect this method to peform better than the previous method of feature extraction.\n",
        "\n",
        "🔵 We have defined a function named `calculate_histogram_features` but it's not complete. It operates on single example of the dataset to calculate the histogram of the image and store it in the column `\"histogram_features\"`. Use `numpy` library (imported as `np`) to calculate the histogram of each image. Replace all the `???` to complete the implementation."
      ],
      "metadata": {
        "id": "Aknzj5NoU7Bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_histogram_features(example, n_bins=16):\n",
        "  img_hist, _ = np.???(example[\"image\"], bins=n_bins, range=(0, 255))\n",
        "  # We normalize the histogram values so it would be independent of the size of image.\n",
        "  img_hist = img_hist / img_hist.sum()\n",
        "  example[\"histogram_features\"] = img_hist\n",
        "  return example"
      ],
      "metadata": {
        "id": "mqHeTOflU6qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔵 Now, use the above function and apply it on train and test images using `map` method to calculate their histogram features. Replace `???` with your answers."
      ],
      "metadata": {
        "id": "FiCZJpmJ1Elj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ???_dataset.???(???)\n",
        "test_dataset = ???_dataset.???(???)"
      ],
      "metadata": {
        "id": "CGc_dCC91Els"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶ Make sure we have correctly implemented the feature extraction step. We should see \"Well done!\" printed without any errors."
      ],
      "metadata": {
        "id": "gsL_sl3m1Elt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(train_dataset[\"histogram_features\"][0]) == 16\n",
        "assert len(test_dataset[\"histogram_features\"][0]) == 16\n",
        "print(\"Well done!\")"
      ],
      "metadata": {
        "id": "Oncy2bJv1Elt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⭕ Train and Evaluate Models on Histogram Features\n",
        "\n",
        "▶ Like the simple features, we would like to evaluate the performance of the models using histogram features we just calculated."
      ],
      "metadata": {
        "id": "MaDnKJRs1Elu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔵 KNN Classifier\n",
        "\n",
        "Train a **KNN** classifier on the extracted features for the training data. We can use the `sklearn` library for this purpose.\n",
        "\n",
        "You can play with the hyper-parameters of the classifier, e.g. `n_neighbors`, and tune them (but it's not necessary)."
      ],
      "metadata": {
        "id": "5qgkBjDZ1Elu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn_classifier = ???\n",
        "???"
      ],
      "metadata": {
        "id": "R5fyDM0p1Elu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔵 Now, find the accuracy of trained classifier on the test data.\n",
        "\n",
        "**Hint:** You can use `score` method."
      ],
      "metadata": {
        "id": "oqMrgZeC1Elu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn_classifier.???(???)"
      ],
      "metadata": {
        "id": "QMy1hVzr1Elu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔵 Decision Tree Classifier\n",
        "\n",
        "Next, we train a **decision tree** classifier on the extracted features for the training data. We can use the `sklearn` library for this purpose.\n",
        "\n",
        "You can play with the hyper-parameters of the classifier, e.g. `max_depth`, and tune them (but it's not necessary)."
      ],
      "metadata": {
        "id": "tmp8Y7Q_1Elv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_classifier = ???\n",
        "???"
      ],
      "metadata": {
        "id": "b0gcWhv51Elv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔵 Like before, we would like to find the accuracy of trained classifier on the test data.\n",
        "\n",
        "**Hint:** Still you can use `score` method!"
      ],
      "metadata": {
        "id": "4R5Blmyb1Elv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_classifier.???(???)"
      ],
      "metadata": {
        "id": "7SWxW5831Elv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔵 Random Forest Classifier\n",
        "\n",
        "And another classifier: we train a **random forest** classifier on the extracted features for the training data. We can use the `sklearn` library for this purpose.\n",
        "\n",
        "You can play with the hyper-parameters of the classifier, e.g. `n_estimators`, and tune them (but it's not necessary)."
      ],
      "metadata": {
        "id": "W7RLQRV81Elv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_classifier = ???\n",
        "???"
      ],
      "metadata": {
        "id": "1_yMYq3O1Elw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔵 And again we need to find the accuracy of trained classifier on the test data."
      ],
      "metadata": {
        "id": "OEEcDUnX1Elw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_classifier.???(???)"
      ],
      "metadata": {
        "id": "PKsYJdc61Elw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⭕ Pretrained Model (ResNet) Features Extraction\n",
        "\n",
        "▶ The third and final method we use for feature extraction is based on using a pretrained CNN (Convolutional Neural Network): for each image, we use a pretrained ResNet model to extract its features. In other words, each image would be represented by the features of the last layer of the ResNet model which is equal to 2048 numbers.\n",
        "\n",
        "🔵 We use the HuggingFace Transformers library to download and load the preprocessor as well as the model weights of the **ResNet-50** model which has been already trained on ImageNet dataset. Replace all the `???` to download and load the pretrained preprocessors and model."
      ],
      "metadata": {
        "id": "-dnE9FCR-w6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet50_img_processor = ???.from_pretrained(\"microsoft/resnet-50\")\n",
        "resnet50_model = ???.from_pretrained(\"microsoft/resnet-50\")"
      ],
      "metadata": {
        "id": "tQgsKiSFrWJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶ Next, we define a function to apply these two operations on a batch of images:\n",
        "\n",
        "- Use the ResNet preprocessor to preprocess the images (e.g. resize, normalizing the pixel values, etc.) so that they are ready to be fed to the ResNet model,\n",
        "- Apply the ResNet model on the preprocessed images to extract their features. As you can see, we use the output of last \"pooler layer\" as the features.\n",
        "\n",
        "🔵 Use ResNet preprocessor and model to extract features from the images. For that, you need to complete the implementation of the following function. Replace all the `???` with your answers."
      ],
      "metadata": {
        "id": "WQp0VIWmBfLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_resnet_features(examples):\n",
        "  inputs = ???(examples[\"image\"], return_tensors=\"pt\")\n",
        "  with torch.no_grad():\n",
        "    features = ???(**inputs).pooler_output.squeeze().numpy()\n",
        "  examples[\"resnet50_features\"] = features\n",
        "  return examples"
      ],
      "metadata": {
        "id": "zTtOJCVuAHA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔵 Now, use the above function and apply it on train and test images using `map` method of dataset to extract features using ResNet model. Replace `???` with your answers."
      ],
      "metadata": {
        "id": "_tOtPKuKGJnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ???_dataset.???(???, batched=True, batch_size=64)\n",
        "test_dataset = ???_dataset.???(???, batched=True, batch_size=64)"
      ],
      "metadata": {
        "id": "n8sJPBLgAqUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(train_dataset[0][\"resnet50_features\"]) == 2048\n",
        "assert len(test_dataset[0][\"resnet50_features\"]) == 2048"
      ],
      "metadata": {
        "id": "9TzeHObAUuuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⭕ Train and Evaluate Models on ResNet Features\n",
        "\n",
        "▶ Now that we have extracted features using ResNet model, we can train and evaluated a classifier on these features. **We expect these features to perform much better than the two previous methods for extracting features and that's due to the power of pretrained features and transfer learning.**"
      ],
      "metadata": {
        "id": "GH06zNVAGE14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔵 KNN Classifier\n",
        "\n",
        "Train a **KNN** classifier on the extracted features for the training data. We can use the `sklearn` library for this purpose.\n",
        "\n",
        "You can play with the hyper-parameters of the classifier, e.g. `n_neighbors`, and tune them (but it's not necessary)."
      ],
      "metadata": {
        "id": "aYVJ1mz3GlYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn_classifier = ???\n",
        "???"
      ],
      "metadata": {
        "id": "YV-Cmj7QGlY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔵 Now, find the accuracy of trained classifier on the test data.\n",
        "\n",
        "**Hint:** You can use `score` method."
      ],
      "metadata": {
        "id": "mjFFvcpsGlY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn_classifier.???(???)"
      ],
      "metadata": {
        "id": "Y2QdyYdyGlY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔵 Decision Tree Classifier\n",
        "\n",
        "Next, we train a **decision tree** classifier on the extracted features for the training data. We can use the `sklearn` library for this purpose.\n",
        "\n",
        "You can play with the hyper-parameters of the classifier, e.g. `max_depth`, and tune them (but it's not necessary)."
      ],
      "metadata": {
        "id": "vWJpZ9LXGlY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_classifier = ???\n",
        "???"
      ],
      "metadata": {
        "id": "YOIypwrxGlY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔵 Like before, we would like to find the accuracy of trained classifier on the test data.\n",
        "\n",
        "**Hint:** Still you can use `score` method!"
      ],
      "metadata": {
        "id": "JY3L9opvGlY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_classifier.???(???)"
      ],
      "metadata": {
        "id": "9nvg70W1GlY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔵 Random Forest Classifier\n",
        "\n",
        "And another classifier: we train a **random forest** classifier on the extracted features for the training data. We can use the `sklearn` library for this purpose.\n",
        "\n",
        "You can play with the hyper-parameters of the classifier, e.g. `n_estimators`, and tune them (but it's not necessary)."
      ],
      "metadata": {
        "id": "GDPVeEmYGlY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_classifier = ???\n",
        "???"
      ],
      "metadata": {
        "id": "Yl3kGES5GlY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔵 And again we need to find the accuracy of trained classifier on the test data."
      ],
      "metadata": {
        "id": "PbgGB-RsGlY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_classifier.???(???)"
      ],
      "metadata": {
        "id": "5mIpH1JDGlY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔵 Open Tasks\n",
        "\n",
        "Now, here are some more tasks which you need to answer:\n",
        "\n",
        "1. Implement a different method for extracting features from an image. And then compare its performance with the methods you tried above.\n",
        "\n",
        "2. Use another classifier besides the three classifiers mentioned above. Can you find a more powerful classifier?\n",
        "\n",
        "3. The ResNet is actually a family of models [with different sizes](https://huggingface.co/models?sort=downloads&search=microsoft%2Fresnet-) (e.g. `resnet-50`, `resnet-18`, `resnet-101`) , and its bigger models usually have a higher performance. Use a bigger version of ResNet model (using Transformers library as above) and see if it can improve the performance of classification.\n",
        "\n",
        "4. Create a table which contains the performance of various models as well as different feature extraction methods you have tried in this notebook. Optionally, you can create it as a Pandas Dataframe using the `pandas` library.\n",
        "\n",
        "Congratulations for reaching so far! 👏 We hope you have enjoyed doing this project and learned many things. 🎉 Good luck and have fun! 🙌"
      ],
      "metadata": {
        "id": "Wk8DxXrhJPRE"
      }
    }
  ]
}